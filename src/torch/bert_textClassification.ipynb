{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nimport os\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nfrom transformers import BertModel, BertTokenizer\nimport warnings\nwarnings.filterwarnings('ignore')\n# specify GPU\ndevice = torch.device(\"cuda\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoadingData():\n            \n    def __init__(self):\n        train_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Train\")\n        validation_file_path = os.path.join(\"..\",\"input\",\"nlp-benchmarking-data-for-intent-and-entity\",\"benchmarking_data\",\"Validate\")\n        category_id = 0\n        self.cat_to_intent = {}\n        self.intent_to_cat = {}\n        \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                self.cat_to_intent[category_id] = intent_id\n                self.intent_to_cat[intent_id] = category_id\n                category_id+=1\n        print(self.cat_to_intent)\n        print(self.intent_to_cat)\n        '''Training data'''\n        training_data = list() \n        for dirname, _, filenames in os.walk(train_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                training_data+=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])\n        self.train_data_frame = pd.DataFrame(training_data, columns =['query', 'intent','category'])   \n        \n        self.train_data_frame = self.train_data_frame.sample(frac = 1)\n\n\n        \n        '''Validation data'''\n        validation_data = list()    \n        for dirname, _, filenames in os.walk(validation_file_path):\n            for filename in filenames:\n                file_path = os.path.join(dirname, filename)\n                intent_id = filename.replace(\".json\",\"\")\n                validation_data +=self.make_data_for_intent_from_json(file_path,intent_id,self.intent_to_cat[intent_id])                \n        self.validation_data_frame = pd.DataFrame(validation_data, columns =['query', 'intent','category'])\n\n        self.validation_data_frame = self.validation_data_frame.sample(frac = 1)\n        \n        \n    def make_data_for_intent_from_json(self,json_file,intent_id,cat):\n        json_d = json.load(open(json_file))         \n        \n        json_dict = json_d[intent_id]\n\n        sent_list = list()\n        for i in json_dict:\n            each_list = i['data']\n            sent =\"\"\n            for i in each_list:\n                sent = sent + i['text']+ \" \"\n            sent =sent[:-1]\n            for i in range(3):\n                sent = sent.replace(\"  \",\" \")\n            sent_list.append((sent,intent_id,cat))\n        return sent_list","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ld = LoadingData()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = ld.train_data_frame","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_map,id2label = ld.intent_to_cat,ld.cat_to_intent","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text, val_text, train_labels, val_labels = train_test_split(train_df['query'], train_df['category'], \n                                                                    random_state=2018, \n                                                                    test_size=0.2, \n                                                                    stratify=train_df['category'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)\nmax_seq_len = max(seq_len)\nprint(max_seq_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize and encode sequences in the training set\nif max_seq_len>512:\n    max_seq_len = 512\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for train set\ntrain_seq = torch.tensor(tokens_train['input_ids'])\ntrain_mask = torch.tensor(tokens_train['attention_mask'])\ntrain_y = torch.tensor(train_labels.tolist())\nprint(\"train_y:\",train_y)\n# for validation set\nval_seq = torch.tensor(tokens_val['input_ids'])\nval_mask = torch.tensor(tokens_val['attention_mask'])\nval_y = torch.tensor(val_labels.tolist())\nprint(\"val_y:\",val_y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 16\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n    def __init__(self, bert,label_map):\n        super(BERT_Arch, self).__init__()\n        self.bert = bert \n      \n        # dropout layer\n        self.dropout = nn.Dropout(0.1)\n\n        # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n\n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,len(label_map))\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n        #define the forward pass\n    def forward(self, sent_id, mask):\n\n        #pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n\n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n\n        # apply softmax activation\n        x = self.softmax(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert,label_map)\n\n# push the model to GPU\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = 1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n\nprint(class_wts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to train the model\ndef train():\n    model.train()\n\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds=[]\n    total_labels =[]\n  \n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n    \n        # progress update after every 50 batches.\n        if step % 100 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n\n        sent_id, mask, labels = batch\n\n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds = preds.detach().cpu().numpy()\n        preds = np.argmax(preds, axis=1)\n        # append the model predictions\n        total_preds+=list(preds)\n        total_labels+=labels.tolist()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss / len(train_dataloader)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    #total_preds  = np.concatenate(total_preds, axis=0)\n    f1 = f1_score(total_labels, total_preds, average='weighted')\n    #returns the loss and predictions\n    return avg_loss, f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n    total_labels = []\n    # iterate over batches\n    for step,batch in enumerate(val_dataloader):\n    \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n\n          # Calculate elapsed time in minutes.\n          #elapsed = format_time(time.time() - t0)\n\n          # Report progress.\n          print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n            preds = np.argmax(preds, axis=1)\n            total_preds+=list(preds)\n            total_labels+=labels.tolist()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss / len(val_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    #total_preds  = np.concatenate(total_preds, axis=0)\n    \n    f1 = f1_score(total_labels, total_preds, average='weighted')\n    return avg_loss, f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(filename, epoch, model, optimizer, label_map, id2label):\n    state = {\n        'epoch': epoch,\n        'model': model,\n        'optimizer': optimizer,\n        'label_map': label_map,\n        'id_map':id2label}\n    torch.save(state, filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, f1_train = train()\n    \n    #evaluate model\n    valid_loss, f1_valid = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        file_name = 'topic_saved_weights.pt'\n        save_checkpoint(file_name, epoch, model, optimizer, label_map, id2label)\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')\n    print(f'\\nTraining F1: {f1_train:.3f}')\n    print(f'Validation F1: {f1_valid:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = 'topic_saved_weights.pt'\ntest_df = ld.validation_data_frame\n\ncheckpoint = torch.load(path,map_location=device)\nmodel = checkpoint.get(\"model\")\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n\n# tokenize and encode sequences in the test set\ntest_text,test_labels = test_df[\"query\"],test_df[\"category\"]\n\ntokens_test = tokenizer.batch_encode_plus(\n    test_text.tolist(),\n    max_length = max_seq_len,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# for test set\ntest_seq = torch.tensor(tokens_test['input_ids'])\ntest_mask = torch.tensor(tokens_test['attention_mask'])\ntest_y = torch.tensor(test_labels.tolist())\nprint(\"test_y:\",test_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions for test data\nwith torch.no_grad():\n    preds = model(test_seq.to(device), test_mask.to(device))\n    preds = preds.detach().cpu().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = np.argmax(preds, axis = 1)\nprint(classification_report(test_y, preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Prediction:\n    def __init__(self):\n        path = 'topic_saved_weights.pt'\n\n        checkpoint = torch.load(path,map_location=device)\n        self.predictor = checkpoint.get(\"model\")\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.tag = checkpoint.get(\"id_map\")\n\n    def predict(self,text):\n        tokens = self.tokenizer.tokenize(text)\n        tokens = tokens[:max_seq_len - 2]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        input_ids = input_ids + [0] * (max_seq_len-len(input_ids))\n        input_ids = torch.tensor(input_ids).unsqueeze(0)\n        input_ids = input_ids.to(device)\n\n        input_mask = [1]*len(tokens) + [0] * (max_seq_len - len(tokens))\n        input_mask = torch.tensor(input_mask).unsqueeze(0)\n        input_mask = input_mask.to(device)\n\n        logits = self.predictor(input_ids,input_mask)\n        prob = torch.nn.functional.softmax(logits,dim=1)\n        result = [(self.tag[idx],item *100) for idx,item in enumerate(prob[0].tolist())]\n        preds = logits.detach().cpu().numpy()\n        pred_val = np.argmax(preds)\n        pred_val = self.tag[pred_val]\n        return result,pred_val\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = Prediction()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_input = [\"Play music from my relentless playlist\",\n             \"I rate this essay a four of 6\"]\n\nfor item in list_input:\n    confidence,pred_val = pred.predict(item)\n    print(pred_val)\n    print(confidence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}